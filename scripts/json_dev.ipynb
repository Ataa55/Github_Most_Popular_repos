{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d89b5-6309-4e82-abb0-015c11bf6765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; } </style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03982d9-fecd-494a-b184-a3969cd37f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "import config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ## glopal variables\n",
    "    spark_master = \"spark://spark-service:7077\"\n",
    "    app_name = \"extratct json data\"\n",
    "    memory = \"4g\"\n",
    "    data_path = \"/home/data/archive/\"\n",
    "    \n",
    "    spark = start_spark_session(spark_master,app_name,memory)\n",
    "\n",
    "    repos_schema = T.StructType([T.StructField('id', T.LongType(), True),\n",
    "                                 T.StructField('repo_name', T.StringType(), True),\n",
    "                                 T.StructField('full_name', T.StringType(), True),\n",
    "                                 T.StructField('description', T.StringType(), True),\n",
    "                                 T.StructField('created', T.StringType(), True), \n",
    "                                 T.StructField('language', T.StringType(), True), \n",
    "                                 T.StructField('type', T.StringType(), True),\n",
    "                                 T.StructField('username', T.StringType(), True),\n",
    "                                 T.StructField('stars', T.LongType(), True),\n",
    "                                 T.StructField('forks', T.LongType(), True), \n",
    "                                 T.StructField('subscribers', T.LongType(), True),\n",
    "                                 T.StructField('open_issues', T.LongType(), True), \n",
    "                                 T.StructField('topics', T.ArrayType(T.StringType(), True), True)\n",
    "                                 ])\n",
    "    df_source_repos = spark.read.json(\"/home/data/archive/\", schema = repos_schema)\n",
    "\n",
    "    \"\"\" Create a table for programming languages called \"programming_lang\" which has two columns, \n",
    "        the programming language name and the number of repos using it.\"\"\"\n",
    "    \n",
    "    programming_lang_df = df_source_repos.groupby(\"language\")\\\n",
    "                                         .agg(F.count(\"*\").alias(\"repo_count\"))\n",
    "    write_sdf_to_postgres_db(programming_lang_df, config.POSTGRS_CREDENTIALS,\"programming_lang\", mode = \"overwrite\")\n",
    "    \n",
    "    \"\"\" Create a table for the organization-type accounts called \"organizations_stars\" which has two columns, \n",
    "        the organization name and the total number of stars across all of its repos in all the files.\"\"\"\n",
    "    \n",
    "    organizations_stars_df = df_source_repos.filter(F.col(\"type\")==\"Organization\")\\\n",
    "                                            .groupby(\"username\").agg(F.sum(\"stars\").alias(\"stars_count\"))\n",
    "\n",
    "    write_sdf_to_postgres_db(organizations_stars_df, config.POSTGRS_CREDENTIALS,\"organizations_stars\", mode = \"overwrite\")\n",
    "\n",
    "\n",
    "    \"\"\" Create a table for the search terms called \"search_terms_relevance\" which has two columns,\n",
    "        the search term - a.k.a. the file name - and the relevance score for all the repos for this search term. \n",
    "        We use a self-defined formular for calculating the relevance \n",
    "        where relevance score = 1.5 * forks + 1.32 * subscribers + 1.04 * stars.\n",
    "    \"\"\"\n",
    "    \n",
    "    relevence_calc = 1.5 * F.col(\"forks\") + 1.32 * F.col(\"subscribers\") +  1.04 * F.col(\"stars\")\n",
    "    \n",
    "    search_terms_relevance_df = df_source_repos.withColumn(\"relevance_score\", \n",
    "                                                           F.round(relevence_calc / 100) )\\\n",
    "                                                           .select(\"repo_name\",\"relevance_score\")\n",
    "\n",
    "    \n",
    "    write_sdf_to_postgres_db(search_terms_relevance_df, config.POSTGRS_CREDENTIALS,\"search_terms_relevance\", mode = \"overwrite\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "542bfb88-b0be-43ec-aa63-abfee57e4240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|           repo_name|relevance_score|\n",
      "+--------------------+---------------+\n",
      "|               spark|          744.0|\n",
      "|             horovod|          162.0|\n",
      "|      SparkInternals|           84.0|\n",
      "|               delta|           61.0|\n",
      "|   TensorFlowOnSpark|           57.0|\n",
      "|              koalas|           40.0|\n",
      "|     spark-jobserver|           47.0|\n",
      "|       analytics-zoo|           38.0|\n",
      "|            ballista|           27.0|\n",
      "|               deequ|           29.0|\n",
      "|       TransmogrifAI|           29.0|\n",
      "|                vega|           25.0|\n",
      "| spark-deep-learning|           29.0|\n",
      "|spark-on-k8s-oper...|           35.0|\n",
      "|                oryx|           28.0|\n",
      "|               spark|           23.0|\n",
      "|        docker-spark|           28.0|\n",
      "|          elassandra|           21.0|\n",
      "|  spark-py-notebooks|           30.0|\n",
      "|          carbondata|           26.0|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_terms_relevance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fa605-eaa8-4a9f-9bed-225bac9fe7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
